{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_files(file_path):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(file_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(file_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                words = re.split(r'\\W+',text)\n",
    "                lower_words = [word.lower() for word in words ]\n",
    "                re_stop_words = [word for word in lower_words if not word in stop_words]\n",
    "                stem_words = [porter.stem(word) for word in re_stop_words]\n",
    "                all_texts.append(\" \".join(stem_words))\n",
    "    return all_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = clean_text_for_files('data/train/pos')\n",
    "# print(max_tokens)\n",
    "neg_train = clean_text_for_files('data/train/neg')\n",
    "# print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels_train = [1] * len(pos_train)\n",
    "neg_labels_train = [0] * len(neg_train)\n",
    "\n",
    "all_text_train = pos_train + neg_train \n",
    "all_text_labels_train = np.array(pos_labels_train +neg_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = clean_text_for_files('data/test/pos')\n",
    "# print(max_tokens)\n",
    "neg_test = clean_text_for_files('data/test/neg')\n",
    "# print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels_test = [1] * len(pos_test)\n",
    "neg_labels_test = [0] * len(neg_test)\n",
    "\n",
    "all_text_test = pos_test + neg_test\n",
    "all_labels_test = np.array(pos_labels_test + neg_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100_000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(all_text_train)\n",
    "sequences = tokenizer.texts_to_sequences(all_text_train)\n",
    "\n",
    "padd_sequences = pad_sequences(sequences, maxlen = max_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocabulary = set(word_index.keys())\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(padd_sequences, all_text_labels_train, test_size = 0.2, random_state=42)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(all_text_test)\n",
    "\n",
    "padd_sequences_test = pad_sequences(sequences_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Ensure that both y_true and y_pred are cast to float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.round(y_pred)  # Round predicted probabilities to 0 or 1\n",
    "    \n",
    "    # True positives, false positives, false negatives\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, 'float32'), axis=0)\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, 'float32'), axis=0)\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), 'float32'), axis=0)\n",
    "\n",
    "    # Precision and recall\n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "    # F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return tf.reduce_mean(f1)\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    # Ensure that both y_true and y_pred are cast to float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.round(y_pred)  # Round predicted probabilities to 0 or 1\n",
    "    \n",
    "    # True positives, false positives, false negatives\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, 'float32'), axis=0)\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, 'float32'), axis=0)\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), 'float32'), axis=0)\n",
    "\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "    return tf.reduce_mean(recall)\n",
    "\n",
    "def prcision_score(y_true, y_pred):\n",
    "    # Ensure that both y_true and y_pred are cast to float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.round(y_pred)  # Round predicted probabilities to 0 or 1\n",
    "    \n",
    "    # True positives, false positives, false negatives\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, 'float32'), axis=0)\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, 'float32'), axis=0)\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), 'float32'), axis=0)\n",
    "\n",
    "    # Precision and recall\n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "\n",
    "    return tf.reduce_mean(precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.6098 - f1_score: 0.3119 - loss: 0.6232 - prcision_score: 0.5004 - recall_score: 0.2354 - val_accuracy: 0.8494 - val_f1_score: 0.3347 - val_loss: 0.3534 - val_prcision_score: 0.5030 - val_recall_score: 0.2517\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9169 - f1_score: 0.3374 - loss: 0.2235 - prcision_score: 0.5028 - recall_score: 0.2549 - val_accuracy: 0.8560 - val_f1_score: 0.3270 - val_loss: 0.3666 - val_prcision_score: 0.5030 - val_recall_score: 0.2431\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9824 - f1_score: 0.3308 - loss: 0.0570 - prcision_score: 0.4967 - recall_score: 0.2490 - val_accuracy: 0.8566 - val_f1_score: 0.3392 - val_loss: 0.5124 - val_prcision_score: 0.5030 - val_recall_score: 0.2568\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9932 - f1_score: 0.3321 - loss: 0.0212 - prcision_score: 0.4972 - recall_score: 0.2504 - val_accuracy: 0.8452 - val_f1_score: 0.3268 - val_loss: 0.7186 - val_prcision_score: 0.5030 - val_recall_score: 0.2430\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9986 - f1_score: 0.3321 - loss: 0.0049 - prcision_score: 0.4975 - recall_score: 0.2500 - val_accuracy: 0.8192 - val_f1_score: 0.3470 - val_loss: 0.7904 - val_prcision_score: 0.5030 - val_recall_score: 0.2660\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9984 - f1_score: 0.3368 - loss: 0.0058 - prcision_score: 0.5018 - recall_score: 0.2542 - val_accuracy: 0.8444 - val_f1_score: 0.3390 - val_loss: 0.7753 - val_prcision_score: 0.5030 - val_recall_score: 0.2566\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9962 - f1_score: 0.3351 - loss: 0.0113 - prcision_score: 0.5005 - recall_score: 0.2527 - val_accuracy: 0.8078 - val_f1_score: 0.3370 - val_loss: 0.8334 - val_prcision_score: 0.5030 - val_recall_score: 0.2542\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9950 - f1_score: 0.3358 - loss: 0.0139 - prcision_score: 0.5011 - recall_score: 0.2532 - val_accuracy: 0.8126 - val_f1_score: 0.3176 - val_loss: 0.8136 - val_prcision_score: 0.5030 - val_recall_score: 0.2330\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9981 - f1_score: 0.3281 - loss: 0.0063 - prcision_score: 0.4938 - recall_score: 0.2465 - val_accuracy: 0.8010 - val_f1_score: 0.3336 - val_loss: 0.9499 - val_prcision_score: 0.5030 - val_recall_score: 0.2504\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9960 - f1_score: 0.3326 - loss: 0.0129 - prcision_score: 0.4979 - recall_score: 0.2505 - val_accuracy: 0.8268 - val_f1_score: 0.3461 - val_loss: 0.9118 - val_prcision_score: 0.5030 - val_recall_score: 0.2649\n"
     ]
    }
   ],
   "source": [
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_len))\n",
    "model.add(SimpleRNN(units=64, return_sequences=False))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',f1_score, recall_score, prcision_score])\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8143 - f1_score: 0.7575 - loss: 0.9920 - prcision_score: 0.8455 - recall_score: 0.6884\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy,f1,precision, recall = model.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5327 - loss: 0.6804 - val_accuracy: 0.8152 - val_loss: 0.4279\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8425 - loss: 0.3764 - val_accuracy: 0.8276 - val_loss: 0.4090\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8896 - loss: 0.2786 - val_accuracy: 0.8240 - val_loss: 0.5019\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9262 - loss: 0.1991 - val_accuracy: 0.8140 - val_loss: 0.4590\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9553 - loss: 0.1240 - val_accuracy: 0.8154 - val_loss: 0.6332\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9799 - loss: 0.0609 - val_accuracy: 0.8056 - val_loss: 0.6781\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9774 - loss: 0.0617 - val_accuracy: 0.8132 - val_loss: 0.7696\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9937 - loss: 0.0206 - val_accuracy: 0.8162 - val_loss: 0.8410\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9917 - loss: 0.0262 - val_accuracy: 0.7958 - val_loss: 0.8415\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9924 - loss: 0.0209 - val_accuracy: 0.7958 - val_loss: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size (based on tokenizer) and embedding dimension\n",
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)  # Cap the vocabulary size\n",
    "embedding_dim = 50  # Dimension of embedding vector\n",
    "\n",
    "# Build the RNN model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model_1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# Simple RNN layer\n",
    "model_1.add(SimpleRNN(units=100, return_sequences=False))\n",
    "\n",
    "# Fully connected layer\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_1.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 1.0914\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_1.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - accuracy: 0.5567 - loss: 0.6742 - val_accuracy: 0.5514 - val_loss: 0.9086\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.6807 - loss: 0.5922 - val_accuracy: 0.6926 - val_loss: 0.6960\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 49ms/step - accuracy: 0.8612 - loss: 0.3354 - val_accuracy: 0.8056 - val_loss: 0.4643\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.9446 - loss: 0.1575 - val_accuracy: 0.7958 - val_loss: 0.5416\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9672 - loss: 0.0978 - val_accuracy: 0.8052 - val_loss: 0.6851\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.9825 - loss: 0.0577 - val_accuracy: 0.7944 - val_loss: 0.7830\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9963 - loss: 0.0161 - val_accuracy: 0.7912 - val_loss: 0.9723\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.9992 - loss: 0.0043 - val_accuracy: 0.7782 - val_loss: 1.1366\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.9998 - loss: 0.0020 - val_accuracy: 0.7898 - val_loss: 1.1930\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 9.4739e-04 - val_accuracy: 0.7938 - val_loss: 1.2465\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 150\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model_2.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# Simple RNN layer\n",
    "model_2.add(SimpleRNN(units=100, return_sequences=False))\n",
    "\n",
    "# Fully connected layer\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_2.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7743 - loss: 1.3375\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_2.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6529 - loss: 0.5858 - val_accuracy: 0.8378 - val_loss: 0.3878\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.9139 - loss: 0.2229 - val_accuracy: 0.8488 - val_loss: 0.3733\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9824 - loss: 0.0526 - val_accuracy: 0.8432 - val_loss: 0.5446\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9960 - loss: 0.0141 - val_accuracy: 0.8012 - val_loss: 0.6857\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9955 - loss: 0.0157 - val_accuracy: 0.8334 - val_loss: 0.8947\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9947 - loss: 0.0170 - val_accuracy: 0.8248 - val_loss: 0.7374\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9848 - loss: 0.0427 - val_accuracy: 0.8468 - val_loss: 0.6947\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - accuracy: 0.9965 - loss: 0.0118 - val_accuracy: 0.8344 - val_loss: 0.7920\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 0.8000 - val_loss: 0.9857\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.9998 - loss: 0.0017 - val_accuracy: 0.8272 - val_loss: 1.0048\n"
     ]
    }
   ],
   "source": [
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 150\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "model_3.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_len))\n",
    "model_3.add(SimpleRNN(units=100, return_sequences=False))\n",
    "model_3.add(Dense(100, activation='relu'))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_3.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8187 - loss: 1.0183\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_3.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
