{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_files(file_path):\n",
    "    all_texts = []\n",
    "    max_tokens = 0\n",
    "    for filename in os.listdir(file_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(file_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                words = re.split(r'\\W+',text)\n",
    "                lower_words = [word.lower() for word in words ]\n",
    "                re_stop_words = [word for word in lower_words if not word in stop_words]\n",
    "                stem_words = [porter.stem(word) for word in re_stop_words]\n",
    "                # if len(stem_words) > max_tokens:\n",
    "                #     max_tokens = len(stem_words)\n",
    "                all_texts.append(\" \".join(stem_words))\n",
    "    return all_texts #, max_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = clean_text_for_files('data/train/pos')\n",
    "# print(max_tokens)\n",
    "neg_train = clean_text_for_files('data/train/neg')\n",
    "# print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels_train = [1] * len(pos_train)\n",
    "neg_labels_train = [0] * len(neg_train)\n",
    "\n",
    "all_text_train = pos_train + neg_train \n",
    "all_text_labels_train = np.array(pos_labels_train +neg_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = clean_text_for_files('data/test/pos')\n",
    "# print(max_tokens)\n",
    "neg_test = clean_text_for_files('data/test/neg')\n",
    "# print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels_test = [1] * len(pos_test)\n",
    "neg_labels_test = [0] * len(neg_test)\n",
    "\n",
    "all_text_test = pos_test + neg_test\n",
    "all_labels_test = np.array(pos_labels_test + neg_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100_000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(all_text_train)\n",
    "sequences = tokenizer.texts_to_sequences(all_text_train)\n",
    "\n",
    "padd_sequences = pad_sequences(sequences, maxlen = max_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocabulary = set(word_index.keys())\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(padd_sequences, all_text_labels_train, test_size = 0.2, random_state=42)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(all_text_test)\n",
    "\n",
    "padd_sequences_test = pad_sequences(sequences_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6322 - loss: 0.6003 - val_accuracy: 0.8644 - val_loss: 0.3181\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9293 - loss: 0.2010 - val_accuracy: 0.8634 - val_loss: 0.3621\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9784 - loss: 0.0711 - val_accuracy: 0.8406 - val_loss: 0.5051\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9921 - loss: 0.0255 - val_accuracy: 0.8470 - val_loss: 0.6293\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9989 - loss: 0.0051 - val_accuracy: 0.8540 - val_loss: 0.7870\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9987 - loss: 0.0049 - val_accuracy: 0.8226 - val_loss: 0.8032\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9963 - loss: 0.0109 - val_accuracy: 0.8306 - val_loss: 0.7852\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9964 - loss: 0.0092 - val_accuracy: 0.8422 - val_loss: 0.8733\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9961 - loss: 0.0110 - val_accuracy: 0.7644 - val_loss: 0.9094\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9938 - loss: 0.0175 - val_accuracy: 0.8228 - val_loss: 0.8836\n"
     ]
    }
   ],
   "source": [
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_len))\n",
    "model.add(SimpleRNN(units=64, return_sequences=False))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8147 - loss: 0.9050\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5327 - loss: 0.6804 - val_accuracy: 0.8152 - val_loss: 0.4279\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8425 - loss: 0.3764 - val_accuracy: 0.8276 - val_loss: 0.4090\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8896 - loss: 0.2786 - val_accuracy: 0.8240 - val_loss: 0.5019\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9262 - loss: 0.1991 - val_accuracy: 0.8140 - val_loss: 0.4590\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9553 - loss: 0.1240 - val_accuracy: 0.8154 - val_loss: 0.6332\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9799 - loss: 0.0609 - val_accuracy: 0.8056 - val_loss: 0.6781\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9774 - loss: 0.0617 - val_accuracy: 0.8132 - val_loss: 0.7696\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9937 - loss: 0.0206 - val_accuracy: 0.8162 - val_loss: 0.8410\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9917 - loss: 0.0262 - val_accuracy: 0.7958 - val_loss: 0.8415\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9924 - loss: 0.0209 - val_accuracy: 0.7958 - val_loss: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size (based on tokenizer) and embedding dimension\n",
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)  # Cap the vocabulary size\n",
    "embedding_dim = 50  # Dimension of embedding vector\n",
    "\n",
    "# Build the RNN model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model_1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# Simple RNN layer\n",
    "model_1.add(SimpleRNN(units=100, return_sequences=False))\n",
    "\n",
    "# Fully connected layer\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_1.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 1.0914\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_1.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - accuracy: 0.5567 - loss: 0.6742 - val_accuracy: 0.5514 - val_loss: 0.9086\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.6807 - loss: 0.5922 - val_accuracy: 0.6926 - val_loss: 0.6960\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 49ms/step - accuracy: 0.8612 - loss: 0.3354 - val_accuracy: 0.8056 - val_loss: 0.4643\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.9446 - loss: 0.1575 - val_accuracy: 0.7958 - val_loss: 0.5416\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9672 - loss: 0.0978 - val_accuracy: 0.8052 - val_loss: 0.6851\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.9825 - loss: 0.0577 - val_accuracy: 0.7944 - val_loss: 0.7830\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9963 - loss: 0.0161 - val_accuracy: 0.7912 - val_loss: 0.9723\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.9992 - loss: 0.0043 - val_accuracy: 0.7782 - val_loss: 1.1366\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.9998 - loss: 0.0020 - val_accuracy: 0.7898 - val_loss: 1.1930\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 9.4739e-04 - val_accuracy: 0.7938 - val_loss: 1.2465\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 150\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model_2.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# Simple RNN layer\n",
    "model_2.add(SimpleRNN(units=100, return_sequences=False))\n",
    "\n",
    "# Fully connected layer\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_2.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7743 - loss: 1.3375\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_2.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6529 - loss: 0.5858 - val_accuracy: 0.8378 - val_loss: 0.3878\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.9139 - loss: 0.2229 - val_accuracy: 0.8488 - val_loss: 0.3733\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9824 - loss: 0.0526 - val_accuracy: 0.8432 - val_loss: 0.5446\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9960 - loss: 0.0141 - val_accuracy: 0.8012 - val_loss: 0.6857\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9955 - loss: 0.0157 - val_accuracy: 0.8334 - val_loss: 0.8947\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9947 - loss: 0.0170 - val_accuracy: 0.8248 - val_loss: 0.7374\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9848 - loss: 0.0427 - val_accuracy: 0.8468 - val_loss: 0.6947\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - accuracy: 0.9965 - loss: 0.0118 - val_accuracy: 0.8344 - val_loss: 0.7920\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 0.8000 - val_loss: 0.9857\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.9998 - loss: 0.0017 - val_accuracy: 0.8272 - val_loss: 1.0048\n"
     ]
    }
   ],
   "source": [
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 150\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "model_3.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_len))\n",
    "model_3.add(SimpleRNN(units=100, return_sequences=False))\n",
    "model_3.add(Dense(100, activation='relu'))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_3.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=100, validation_data=(np.array(X_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8187 - loss: 1.0183\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_3.evaluate(padd_sequences_test, all_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
